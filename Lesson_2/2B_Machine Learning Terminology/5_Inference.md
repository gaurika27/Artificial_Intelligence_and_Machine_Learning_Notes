Inference is the process of making predictions using a trained model. In deep learning applications, inference accounts for up to 90 percent of total operational costs for two reasons. First, standalone graphics processing unit (GPU) instances are typically designed for model trainingâ€”not for inference. Though training jobs batch process hundreds of data samples in parallel, inference jobs usually process a single input in real time. Thus, they consume a small amount of GPU compute, which makes standalone GPU inference cost-inefficient. On the other hand, standalone CPU instances are not specialized for matrix operations, and thus are often too slow for deep learning inference.

Secondly, different models have different CPU, GPU, and memory requirements. Optimizing for one resource can lead to underutilization of other resources and higher costs. Amazon Web Services (AWS) resolves these cost-inefficient issues with Amazon Elastic Inference. You will learn more about Amazon Elastic Inference and when to use it in the next module.
